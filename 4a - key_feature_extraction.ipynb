{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "from plotting_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/acraig/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/acraig/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/acraig/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words = 10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \" Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_pickle('20191128_goodreads_book_data.pkl').join(\n",
    "        pd.read_pickle('20191128_google_book_data.pkl')[['isbn', 'categories', 'description']].add_prefix('google_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(index = books.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Goodreads shelves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shelves = pd.get_dummies(books.goodreads_shelves.apply(pd.Series).stack()).sum(level=0)\n",
    "shelves = shelves.reindex(books.index)\n",
    "features = features.join(shelves.add_prefix('shelf_'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Google books categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = pd.get_dummies(books.google_categories.apply(pd.Series).stack()).sum(level=0)\n",
    "categories = categories.reindex(books.index)\n",
    "features = features.join(categories.add_prefix('category_'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse descriptions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare textual data\n",
    "alphanumeric_filter = re.compile(r'[\\W_-]+')\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize(text, name):\n",
    "    names = name.strip().replace(',', '').split(' ')\n",
    "    for n in names:\n",
    "        text = text.replace(n, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['gr_description'] =books[['goodreads_description', 'Author']].fillna('')\\\n",
    "                                    .apply(lambda r: anonymize(r['goodreads_description'].lower(),\n",
    "                                                               r['Author'].lower()), axis =1)\n",
    "\n",
    "books['gr_description'] =books.gr_description.fillna('')\\\n",
    "                                     .map(lambda text: alphanumeric_filter.sub(' ', text).lower().strip())\\\n",
    "                                     .map(lambda text: ' '.join([wnl.lemmatize(token) for token in wpt.tokenize(text) \n",
    "                                                                 if (token not in stop_words) and \n",
    "                                                                    (nltk.pos_tag([token])[0][1] == 'NN') and \n",
    "                                                                    (len(token)> 2)\n",
    "                                                               ]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['gb_description'] =books[['google_description', 'Author']].fillna('')\\\n",
    "                                    .apply(lambda r: anonymize(r['google_description'].lower(),\n",
    "                                                               r['Author'].lower()), axis =1)\n",
    "\n",
    "books['gb_description'] =books.gb_description.fillna('')\\\n",
    "                                     .map(lambda text: alphanumeric_filter.sub(' ', text).lower().strip())\\\n",
    "                                     .map(lambda text: ' '.join([wnl.lemmatize(token) for token in wpt.tokenize(text) \n",
    "                                                                 if (token not in stop_words) and \n",
    "                                                                    (nltk.pos_tag([token])[0][1] == 'NN') and \n",
    "                                                                    (len(token)> 2)\n",
    "                                                               ]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse bios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare textual data\n",
    "alphanumeric_filter = re.compile(r'[\\W_]+')\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "books['gr_bio'] =books.goodreads_author_bio.fillna('')\\\n",
    "                                     .map(lambda x: alphanumeric_filter.sub(' ', x).lower().strip())\\\n",
    "                                     .map(lambda text: ' '.join([wnl.lemmatize(token) for token in wpt.tokenize(text) \n",
    "                                                                 if (token not in stop_words) and \n",
    "                                                                    (nltk.pos_tag([token])[0][1] == 'NN') and \n",
    "                                                                    (len(token)> 2)\n",
    "                                                               ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine text data & select words of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['text_metadata'] = books[['gr_description', 'gb_description', 'gr_bio']].sum(axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['text_metadata_mapped'] = books['text_metadata'].apply(lambda s: s.replace('sopher', 'sophy')\n",
    "                                                                        .replace('trist', 'try')\n",
    "                                                                        .replace('gist', 'gy')\n",
    "                                                                        .replace('pist', 'py')\n",
    "                                                                        .replace('trist', 'try')\n",
    "                                                                        .replace('scientist', 'science')\n",
    "                                                                        .replace('historian', 'history')\n",
    "                                                                        .replace('physicist', 'physics')\n",
    "                                                                        .replace('chemist', 'chemistry')\n",
    "                                                                        .replace('economist', 'economics'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF-IDF \n",
    "max_df = 1.0\n",
    "n = 1000\n",
    "\n",
    "\n",
    "tfidf_v = TfidfVectorizer(use_idf=True, stop_words = ['audiobook', \n",
    "                                                      'new', 'york', 'time', 'times', 'america',\n",
    "                                                      'today', 'year',\n",
    "                                                      'bestselling', 'bestseller', 'award', 'winning', 'prize',\n",
    "                                                      'book', 'series', 'story', 'edition',\n",
    "                                                      'way', 'born', 'work', 'become', 'set', 'include'],\n",
    "                          max_df=max_df,\n",
    "                          binary= True)\n",
    "tfidf_weights = tfidf_v.fit_transform(books['text_metadata_mapped'])\n",
    "\n",
    "weights = tfidf_weights.toarray().sum(axis = 0)\n",
    "weighted_words = tfidf_v.get_feature_names()\n",
    "\n",
    "idx = np.argpartition(weights, -n)[-n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.join(pd.DataFrame(data = tfidf_weights.toarray()[:, idx],\n",
    "                                     columns = [weighted_words[i] for i in idx],\n",
    "                                     index = books.index).add_prefix('text_'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processed publisher data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "publishers= pd.get_dummies(pd.read_pickle('20191128_normalized_publisher.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.join(publishers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A priori filtering of features with little coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 1510)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = (features.sum()/len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = coverage[coverage>=0.005].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shelf_adult',\n",
       " 'shelf_adventure',\n",
       " 'shelf_africa',\n",
       " 'shelf_african-american',\n",
       " 'shelf_american-history',\n",
       " 'shelf_ancient-history',\n",
       " 'shelf_animals',\n",
       " 'shelf_anthropology',\n",
       " 'shelf_archaeology',\n",
       " 'shelf_art',\n",
       " 'shelf_art-history',\n",
       " 'shelf_asia',\n",
       " 'shelf_astronomy',\n",
       " 'shelf_audiobook',\n",
       " 'shelf_australia',\n",
       " 'shelf_autobiography',\n",
       " 'shelf_biography',\n",
       " 'shelf_biography-memoir',\n",
       " 'shelf_biology',\n",
       " 'shelf_birds',\n",
       " 'shelf_books-about-books',\n",
       " 'shelf_brain',\n",
       " 'shelf_buddhism',\n",
       " 'shelf_business',\n",
       " 'shelf_chemistry',\n",
       " 'shelf_china',\n",
       " 'shelf_classics',\n",
       " 'shelf_comedy',\n",
       " 'shelf_cooking',\n",
       " 'shelf_design',\n",
       " 'shelf_dogs',\n",
       " 'shelf_drama',\n",
       " 'shelf_ecology',\n",
       " 'shelf_economics',\n",
       " 'shelf_education',\n",
       " 'shelf_egypt',\n",
       " 'shelf_environment',\n",
       " 'shelf_essays',\n",
       " 'shelf_evolution',\n",
       " 'shelf_fairy-tales',\n",
       " 'shelf_fantasy',\n",
       " 'shelf_feminism',\n",
       " 'shelf_fiction',\n",
       " 'shelf_folklore',\n",
       " 'shelf_food',\n",
       " 'shelf_genetics',\n",
       " 'shelf_geology',\n",
       " 'shelf_health',\n",
       " 'shelf_historical',\n",
       " 'shelf_historical-fiction',\n",
       " 'shelf_history',\n",
       " 'shelf_humor',\n",
       " 'shelf_italy',\n",
       " 'shelf_japan',\n",
       " 'shelf_japanese-literature',\n",
       " 'shelf_language',\n",
       " 'shelf_leadership',\n",
       " 'shelf_lgbt',\n",
       " 'shelf_linguistics',\n",
       " 'shelf_literature',\n",
       " 'shelf_medical',\n",
       " 'shelf_medicine',\n",
       " 'shelf_medieval',\n",
       " 'shelf_medieval-history',\n",
       " 'shelf_memoir',\n",
       " 'shelf_mental-health',\n",
       " 'shelf_microhistory',\n",
       " 'shelf_music',\n",
       " 'shelf_mystery',\n",
       " 'shelf_mythology',\n",
       " 'shelf_natural-history',\n",
       " 'shelf_nature',\n",
       " 'shelf_neuroscience',\n",
       " 'shelf_non-fiction',\n",
       " 'shelf_personal-development',\n",
       " 'shelf_philosophy',\n",
       " 'shelf_physics',\n",
       " 'shelf_plants',\n",
       " 'shelf_plays',\n",
       " 'shelf_poetry',\n",
       " 'shelf_politics',\n",
       " 'shelf_popular-science',\n",
       " 'shelf_psychology',\n",
       " 'shelf_queer',\n",
       " 'shelf_race',\n",
       " 'shelf_reference',\n",
       " 'shelf_religion',\n",
       " 'shelf_school',\n",
       " 'shelf_science',\n",
       " 'shelf_science-nature',\n",
       " 'shelf_self-help',\n",
       " 'shelf_short-stories',\n",
       " 'shelf_social-justice',\n",
       " 'shelf_sociology',\n",
       " 'shelf_spirituality',\n",
       " 'shelf_technology',\n",
       " 'shelf_theatre',\n",
       " 'shelf_travel',\n",
       " 'shelf_war',\n",
       " 'shelf_writing',\n",
       " 'category_Art',\n",
       " 'category_Biography & Autobiography',\n",
       " 'category_Body, Mind & Spirit',\n",
       " 'category_Business & Economics',\n",
       " 'category_Cooking',\n",
       " 'category_Fiction',\n",
       " 'category_Health & Fitness',\n",
       " 'category_History',\n",
       " 'category_Humor',\n",
       " 'category_International Standard Book Numbers',\n",
       " 'category_Language Arts & Disciplines',\n",
       " 'category_Literary Collections',\n",
       " 'category_Literary Criticism',\n",
       " 'category_Medical',\n",
       " 'category_Music',\n",
       " 'category_Nature',\n",
       " 'category_Pets',\n",
       " 'category_Philosophy',\n",
       " 'category_Political Science',\n",
       " 'category_Psychology',\n",
       " 'category_Religion',\n",
       " 'category_Science',\n",
       " 'category_Social Science',\n",
       " 'category_Travel',\n",
       " 'text_europe',\n",
       " 'text_part',\n",
       " 'text_washington',\n",
       " 'text_past',\n",
       " 'text_left',\n",
       " 'text_war',\n",
       " 'text_learn',\n",
       " 'text_childhood',\n",
       " 'text_field',\n",
       " 'text_perfect',\n",
       " 'text_oxford',\n",
       " 'text_perspective',\n",
       " 'text_day',\n",
       " 'text_volume',\n",
       " 'text_language',\n",
       " 'text_death',\n",
       " 'text_fiction',\n",
       " 'text_life',\n",
       " 'text_philosophy',\n",
       " 'text_city',\n",
       " 'text_change',\n",
       " 'text_place',\n",
       " 'text_light',\n",
       " 'text_view',\n",
       " 'text_planet',\n",
       " 'text_century',\n",
       " 'text_play',\n",
       " 'text_culture',\n",
       " 'text_deep',\n",
       " 'text_center',\n",
       " 'text_deeply',\n",
       " 'text_age',\n",
       " 'text_order',\n",
       " 'text_knowledge',\n",
       " 'text_literature',\n",
       " 'text_ancient',\n",
       " 'text_living',\n",
       " 'text_food',\n",
       " 'text_kind',\n",
       " 'text_london',\n",
       " 'text_everything',\n",
       " 'text_post',\n",
       " 'text_variety',\n",
       " 'text_key',\n",
       " 'text_power',\n",
       " 'text_practice',\n",
       " 'text_evidence',\n",
       " 'text_look',\n",
       " 'text_wife',\n",
       " 'text_department',\n",
       " 'text_present',\n",
       " 'text_wikipedia',\n",
       " 'text_press',\n",
       " 'text_fellow',\n",
       " 'text_animal',\n",
       " 'text_use',\n",
       " 'text_form',\n",
       " 'text_process',\n",
       " 'text_professor',\n",
       " 'text_profound',\n",
       " 'text_university',\n",
       " 'text_evolution',\n",
       " 'text_love',\n",
       " 'text_unique',\n",
       " 'text_career',\n",
       " 'text_found',\n",
       " 'text_founder',\n",
       " 'text_journey',\n",
       " 'text_journalist',\n",
       " 'text_journal',\n",
       " 'text_provide',\n",
       " 'text_winner',\n",
       " 'text_number',\n",
       " 'text_understand',\n",
       " 'text_psychology',\n",
       " 'text_john',\n",
       " 'text_developed',\n",
       " 'text_novel',\n",
       " 'text_development',\n",
       " 'text_public',\n",
       " 'text_north',\n",
       " 'text_question',\n",
       " 'text_radio',\n",
       " 'text_twenty',\n",
       " 'text_non',\n",
       " 'text_collection',\n",
       " 'text_california',\n",
       " 'text_create',\n",
       " 'text_college',\n",
       " 'text_anyone',\n",
       " 'text_woman',\n",
       " 'text_range',\n",
       " 'text_turn',\n",
       " 'text_read',\n",
       " 'text_reader',\n",
       " 'text_reading',\n",
       " 'text_business',\n",
       " 'text_director',\n",
       " 'text_magazine',\n",
       " 'text_introduction',\n",
       " 'text_neuroscience',\n",
       " 'text_father',\n",
       " 'text_relationship',\n",
       " 'text_future',\n",
       " 'text_discovery',\n",
       " 'text_research',\n",
       " 'text_world',\n",
       " 'text_intelligence',\n",
       " 'text_england',\n",
       " 'text_area',\n",
       " 'text_need',\n",
       " 'text_result',\n",
       " 'text_course',\n",
       " 'text_review',\n",
       " 'text_insight',\n",
       " 'text_brilliant',\n",
       " 'text_right',\n",
       " 'text_country',\n",
       " 'text_rise',\n",
       " 'text_information',\n",
       " 'text_influence',\n",
       " 'text_nature',\n",
       " 'text_role',\n",
       " 'text_nation',\n",
       " 'text_tour',\n",
       " 'text_family',\n",
       " 'text_brain',\n",
       " 'text_end',\n",
       " 'text_man',\n",
       " 'text_importance',\n",
       " 'text_impact',\n",
       " 'text_community',\n",
       " 'text_name',\n",
       " 'text_school',\n",
       " 'text_science',\n",
       " 'text_art',\n",
       " 'text_told',\n",
       " 'text_music',\n",
       " 'text_body',\n",
       " 'text_writer',\n",
       " 'text_secret',\n",
       " 'text_human',\n",
       " 'text_self',\n",
       " 'text_sense',\n",
       " 'text_fact',\n",
       " 'text_shape',\n",
       " 'text_account',\n",
       " 'text_hope',\n",
       " 'text_mother',\n",
       " 'text_biology',\n",
       " 'text_show',\n",
       " 'text_home',\n",
       " 'text_thought',\n",
       " 'text_simple',\n",
       " 'text_think',\n",
       " 'text_history',\n",
       " 'text_theory',\n",
       " 'text_text',\n",
       " 'text_society',\n",
       " 'text_help',\n",
       " 'text_south',\n",
       " 'text_master',\n",
       " 'text_behavior',\n",
       " 'text_attention',\n",
       " 'text_tell',\n",
       " 'text_experience',\n",
       " 'text_spent',\n",
       " 'text_heart',\n",
       " 'text_health',\n",
       " 'text_beauty',\n",
       " 'text_beautiful',\n",
       " 'text_state',\n",
       " 'text_meaning',\n",
       " 'text_mind',\n",
       " 'text_harvard',\n",
       " 'text_taught',\n",
       " 'text_earth',\n",
       " 'text_author',\n",
       " 'text_medicine',\n",
       " 'text_study',\n",
       " 'text_subject',\n",
       " 'text_edge',\n",
       " 'text_explore',\n",
       " 'text_guide',\n",
       " 'text_system',\n",
       " 'text_exploration',\n",
       " 'text_editor',\n",
       " 'text_surprising',\n",
       " 'text_memoir',\n",
       " 'text_ability',\n",
       " 'text_education',\n",
       " 'text_group',\n",
       " 'text_expert',\n",
       " 'publisher_',\n",
       " 'publisher_Ballantine Books',\n",
       " 'publisher_Bantam',\n",
       " 'publisher_Basic Books',\n",
       " 'publisher_Bloomsbury Sigma',\n",
       " 'publisher_Cambridge University Press',\n",
       " 'publisher_Createspace Independent Publishing Platform',\n",
       " 'publisher_Crown',\n",
       " 'publisher_Dutton',\n",
       " 'publisher_Farrar, Straus and Giroux',\n",
       " 'publisher_Grand Central Publishing',\n",
       " 'publisher_Harper',\n",
       " 'publisher_HarperCollins',\n",
       " 'publisher_Harvard University Press',\n",
       " 'publisher_Henry Holt & Company',\n",
       " 'publisher_Houghton Mifflin Harcourt',\n",
       " 'publisher_Knopf',\n",
       " 'publisher_Little, Brown and Company',\n",
       " 'publisher_Liveright',\n",
       " 'publisher_Oxford University Press, USA',\n",
       " 'publisher_Pegasus Books',\n",
       " 'publisher_Penguin Books',\n",
       " 'publisher_Picador',\n",
       " 'publisher_Princeton University Press',\n",
       " 'publisher_Random House',\n",
       " 'publisher_Riverhead Books',\n",
       " 'publisher_Scribner',\n",
       " 'publisher_Shambhala',\n",
       " 'publisher_Simon & Schuster',\n",
       " 'publisher_Spiegel & Grau',\n",
       " \"publisher_St. Martin's Press\",\n",
       " 'publisher_Twelve',\n",
       " 'publisher_University of California Press',\n",
       " 'publisher_University of Chicago Press',\n",
       " 'publisher_Viking',\n",
       " 'publisher_Vintage',\n",
       " 'publisher_W. W. Norton  Company',\n",
       " 'publisher_William Morrow',\n",
       " 'publisher_Yale University Press']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 356)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[selected_features].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
